# -*- coding: utf-8 -*-
"""Shiva Copy - GP8000 Group A4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m5ddaD2WpO3JgJ8vi7Q638mFCj3o8oN2
"""

# prompt: write a code segment to retrieve api from data.gov.sg

import requests
import json

def get_data_gov_sg_api(api_url):
  """Retrieves data from a Data.gov.sg API endpoint.

  Args:
      api_url: The URL of the Data.gov.sg API endpoint.

  Returns:
      A JSON object containing the API response data, or None if there was an error.
  """
  try:
    response = requests.get(api_url)
    response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)

    data = response.json()
    return data
  except requests.exceptions.RequestException as e:
    print(f"Error fetching data from API: {e}")
    return None

# Example usage: Replace with your desired API endpoint.
api_url = "https://api-open.data.gov.sg/v2/real-time/api/twenty-four-hr-forecast?" #Example API
api_data = get_data_gov_sg_api(api_url)

if api_data:
    # Process the retrieved data
    #print(json.dumps(api_data, indent=2))  # Print formatted JSON
    # Further processing or analysis of the data goes here...
    # Example accessing data:
    # if 'items' in api_data and len(api_data['items']) > 0:
      # print(api_data['items'][0]['readings'])
    print(api_data['data']['records'])

REGION = "central" # @param ["north","south","east","west","central"]


api_url = "https://api-open.data.gov.sg/v2/real-time/api/twenty-four-hr-forecast?" #Example API
api_data = get_data_gov_sg_api(api_url)
data = api_data['data']['records'][0]

if api_data:
    print("Temperature: " + str(data['general']['temperature']['low']) + "°C to " + str(data['general']['temperature']['high']) + "°C")
    print("Humidity: " + str(data['general']['relativeHumidity']['low']) + "% to " + str(data['general']['relativeHumidity']['high']) + "%")
    print("Wind Speed: " + str(data['general']['wind']['speed']['low']) + "km/h to " + str(data['general']['wind']['speed']['high']) + "km/h " + str(data['general']['wind']['direction']))
    print("Forecast: " + data['general']['forecast']['text'])

    print(f"\nForecast for {REGION} side: " + data['periods'][0]['regions'][f'{REGION}']['text'])

# prompt: create a basic linear regression model
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Sample data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

# Create and train the model
model = LinearRegression()
model.fit(X, y)

# Make predictions
X_new = np.array([[6]])
y_pred = model.predict(X_new)

print(f"Prediction for X_new: {y_pred}")

DATASET_ID = "d_03bb2eb67ad645d0188342fa74ad7066" # e.g. "d_69b3380ad7e51aff3a7dcc84eba52b8a"

import json
import requests

s = requests.Session()
s.headers.update({'referer': 'https://colab.research.google.com'})
base_url = "https://api-production.data.gov.sg"
url = base_url + f"/v2/public/api/datasets/{DATASET_ID}/metadata"
print(url)
response = s.get(url)
data = response.json()['data']
columnMetadata = data.pop('columnMetadata', None)

print("Dataset Metadata:")
print(json.dumps(data, indent=2))

print("\nColumns:\n", list(columnMetadata['map'].values()))

import time
import pandas as pd

def download_file(DATASET_ID):
  # initiate download
  initiate_download_response = s.get(
      f"https://api-open.data.gov.sg/v1/public/api/datasets/{DATASET_ID}/initiate-download",
      headers={"Content-Type":"application/json"},
      json={}
  )
  print(initiate_download_response.json()['data']['message'])

  # poll download
  MAX_POLLS = 5
  for i in range(MAX_POLLS):
    poll_download_response = s.get(
        f"https://api-open.data.gov.sg/v1/public/api/datasets/{DATASET_ID}/poll-download",
        headers={"Content-Type":"application/json"},
        json={}
    )
    print("Poll download response:", poll_download_response.json())
    if "url" in poll_download_response.json()['data']:
      print(poll_download_response.json()['data']['url'])
      DOWNLOAD_URL = poll_download_response.json()['data']['url']
      df = pd.read_csv(DOWNLOAD_URL)

      display(df.head())
      print("\nDataframe loaded!")
      return df
    if i == MAX_POLLS - 1:
      print(f"{i+1}/{MAX_POLLS}: No result found, possible error with dataset, please try again or let us know at https://go.gov.sg/datagov-supportform\n")
    else:
      print(f"{i+1}/{MAX_POLLS}: No result yet, continuing to poll\n")
    time.sleep(3)

df = download_file(DATASET_ID)

df.describe()

import numpy as np
import pandas as pd
# import datetime

def add_cyclic_features(df):
    df['date'] = pd.to_datetime(df['date'])
    df['day_of_year'] = df['date'].dt.dayofyear
    df['day_of_week'] = df['date'].dt.dayofweek
    df['month'] = df['date'].dt.month
    # Create cyclic (sin & cos) features for day-of-year, day-of-week, and month.
    df['sin_day_of_year'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
    df['cos_day_of_year'] = np.cos(2 * np.pi * df['day_of_year'] / 365)
    df['sin_day_of_week'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['cos_day_of_week'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)
    df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)
    return df

# Example usage:
# df = pd.read_csv("your_weather_data.csv")
df = add_cyclic_features(df)
print(df.head())

import torch
import torch.nn as nn
import torch.optim as optim

# Specify the target columns.
target_cols = ['maximum_temperature', 'minimum_temperature', 'max_wind_speed']

# Replace 'na' with NaN and drop rows with missing target values.
df = df.replace('na', np.nan)
df = df.dropna(subset=target_cols)

# PyTorch Dataset class using cyclic features as input.
class WeatherDataset(torch.utils.data.Dataset):
    def __init__(self, df):
        # Use the 6 cyclic features as input.
        self.features = df[['sin_day_of_year', 'cos_day_of_year',
                            'sin_day_of_week', 'cos_day_of_week',
                            'sin_month', 'cos_month']].values.astype(np.float32)
        # Targets: maximum_temperature, minimum_temperature, and max_wind_speed.
        self.targets = df[target_cols].values.astype(np.float32)
        self.features = torch.tensor(self.features)  # shape: (N, 6)
        self.targets = torch.tensor(self.targets)      # shape: (N, 3)
        # Reshape features to (N, seq_len=6, 1) so each cyclic value is treated as a token.
        self.features = self.features.unsqueeze(-1)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]

# Define the transformer-based model with dropout and batch_first=True.
class WeatherTransformer(nn.Module):
    def __init__(self, emb_dim=32, nhead=4, num_layers=2, dropout=0.1, output_dim=3):
        super(WeatherTransformer, self).__init__()
        # Input projection: from scalar to embedding vector.
        self.input_proj = nn.Linear(1, emb_dim)
        # Transformer encoder layer with dropout.
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=emb_dim, nhead=nhead, dropout=dropout, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        # Optional dropout before the final FC.
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(emb_dim, output_dim)

    def forward(self, x):
        # x shape: (batch, seq_len, 1) where seq_len is now 6.
        x = self.input_proj(x)  # now (batch, seq_len, emb_dim)
        x = self.transformer_encoder(x)  # (batch, seq_len, emb_dim)
        x = x.mean(dim=1)  # Mean pooling over sequence dimension.
        x = self.dropout(x)
        output = self.fc(x)  # (batch, output_dim)
        return output

# Combined loss: weighted sum of custom loss and traditional MSE loss.
def combined_loss(pred, target, target_mean, target_std, penalty_factor=2.0, alpha=0.5):
    # Custom loss component: penalize predictions, adding extra penalty when |target-mean| > std.
    base_loss = (pred - target)**2
    mask = (torch.abs(target - target_mean) > target_std).float()
    custom_loss_val = (base_loss * (1 + penalty_factor * mask)).mean()

    # Traditional MSE loss.
    mse_loss_val = nn.MSELoss()(pred, target)

    # Combine the two losses.
    return alpha * custom_loss_val + (1 - alpha) * mse_loss_val

# Create the dataset.
dataset = WeatherDataset(df)
print("Total dataset size:", len(dataset))

# Compute per-target mean and std on the full dataset.
target_mean = dataset.targets.mean(dim=0)
target_std = dataset.targets.std(dim=0)
print("Target Mean:", target_mean, "Target Std:", target_std)

!pip install optuna

import optuna
from sklearn.model_selection import KFold
from torch.utils.data import Subset, DataLoader
import math

def objective(trial):
    # Sample nhead first, then compute proper bounds for emb_dim.
    nhead = trial.suggest_int("nhead", 2, 8)
    min_val = math.ceil(16 / nhead) * nhead
    max_val = math.floor(64 / nhead) * nhead
    emb_dim = trial.suggest_int("emb_dim", min_val, max_val, step=nhead)

    num_layers = trial.suggest_int("num_layers", 1, 4)
    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)
    alpha = trial.suggest_float("alpha", 0.3, 0.7)
    penalty_factor = trial.suggest_float("penalty_factor", 1.0, 3.0)
    dropout = trial.suggest_float("dropout", 0.0, 0.3)

    # 5-fold cross validation.
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    val_losses = []

    for train_index, val_index in kf.split(np.arange(len(dataset))):
        train_subset = Subset(dataset, train_index)
        val_subset = Subset(dataset, val_index)

        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)

        # Instantiate new model for each fold.
        model = WeatherTransformer(
            emb_dim=emb_dim, nhead=nhead, num_layers=num_layers,
            dropout=dropout, output_dim=3
        )
        optimizer = optim.Adam(model.parameters(), lr=lr)

        # Train for a few epochs.
        epochs = 20
        for epoch in range(epochs):
            model.train()
            for features, targets in train_loader:
                optimizer.zero_grad()
                outputs = model(features)
                loss = combined_loss(outputs, targets, target_mean, target_std, penalty_factor, alpha)
                loss.backward()
                optimizer.step()

        # Evaluate on validation fold.
        model.eval()
        total_val_loss = 0.0
        with torch.no_grad():
            for features, targets in val_loader:
                outputs = model(features)
                loss = combined_loss(outputs, targets, target_mean, target_std, penalty_factor, alpha)
                total_val_loss += loss.item() * features.size(0)
        avg_val_loss = total_val_loss / len(val_subset)
        val_losses.append(avg_val_loss)

    return np.mean(val_losses)

study = optuna.create_study(direction="minimize", pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=20)

print("Best trial:")
trial = study.best_trial
for key, value in trial.params.items():
    print(f"  {key}: {value}")

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Use best hyperparameters from Optuna (or defaults if not available).
best_params = study.best_trial.params
emb_dim = best_params.get("emb_dim", 32)
nhead = best_params.get("nhead", 4)
num_layers = best_params.get("num_layers", 2)
lr = best_params.get("lr", 0.001)
alpha = best_params.get("alpha", 0.5)
penalty_factor = best_params.get("penalty_factor", 2.0)
dropout = best_params.get("dropout", 0.1)

# Create fixed train-validation split (80-20).
indices = np.arange(len(dataset))
train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)

from torch.utils.data import Subset
train_subset = Subset(dataset, train_idx)
val_subset = Subset(dataset, val_idx)

train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)

# Instantiate final model.
model = WeatherTransformer(emb_dim=emb_dim, nhead=nhead, num_layers=num_layers, dropout=dropout, output_dim=3)
optimizer = optim.Adam(model.parameters(), lr=lr)

# Containers for metrics.
num_epochs = 100
train_losses = []
val_losses = []
train_rmse = []
val_rmse = []
train_mape = []
val_mape = []

# Helper function to compute MAPE.
def compute_mape(pred, target):
    # Avoid division by zero by adding a small epsilon.
    epsilon = 1e-8
    return torch.mean(torch.abs((pred - target) / (target + epsilon))).item()

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    running_mape = 0.0
    for features, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(features)
        loss = combined_loss(outputs, targets, target_mean, target_std, penalty_factor, alpha)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * features.size(0)
        running_mape += compute_mape(outputs, targets) * features.size(0)
    avg_train_loss = running_loss / len(train_subset)
    train_losses.append(avg_train_loss)
    train_rmse.append(np.sqrt(avg_train_loss))
    train_mape.append(running_mape / len(train_subset))

    # Validation evaluation.
    model.eval()
    running_val_loss = 0.0
    running_val_mape = 0.0
    with torch.no_grad():
        for features, targets in val_loader:
            outputs = model(features)
            loss = combined_loss(outputs, targets, target_mean, target_std, penalty_factor, alpha)
            running_val_loss += loss.item() * features.size(0)
            running_val_mape += compute_mape(outputs, targets) * features.size(0)
    avg_val_loss = running_val_loss / len(val_subset)
    val_losses.append(avg_val_loss)
    val_rmse.append(np.sqrt(avg_val_loss))
    val_mape.append(running_val_mape / len(val_subset))

    if (epoch+1) % 10 == 0:
        print(f"Epoch {epoch+1}/{num_epochs}:")
        print(f"  Train Loss: {avg_train_loss:.4f} (RMSE: {train_rmse[-1]:.4f}, MAPE: {train_mape[-1]*100:.2f}%)")
        print(f"  Val Loss:   {avg_val_loss:.4f} (RMSE: {val_rmse[-1]:.4f}, MAPE: {val_mape[-1]*100:.2f}%)")

# Plot training and validation curves.
plt.figure(figsize=(14, 5))

plt.subplot(1, 3, 1)
plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs+1), val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(range(1, num_epochs+1), train_rmse, label='Train RMSE')
plt.plot(range(1, num_epochs+1), val_rmse, label='Val RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.title('RMSE over Epochs')
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(range(1, num_epochs+1), [m*100 for m in train_mape], label='Train MAPE (%)')
plt.plot(range(1, num_epochs+1), [m*100 for m in val_mape], label='Val MAPE (%)')
plt.xlabel('Epoch')
plt.ylabel('MAPE (%)')
plt.title('MAPE over Epochs')
plt.legend()

plt.tight_layout()
plt.show()

# Save the final model.
torch.save(model.state_dict(), 'weather_transformer.pt')

def predict_weather(model, date):
    """
    Given a date string (e.g., "2025-03-28"), compute cyclic features
    for day-of-year, day-of-week, and month, and predict the outputs:
    maximum_temperature, minimum_temperature, and max_wind_speed.
    """
    date_obj = pd.to_datetime(date)
    day_of_year = date_obj.dayofyear
    day_of_week = date_obj.dayofweek
    month = date_obj.month

    sin_day_of_year = np.sin(2 * np.pi * day_of_year / 365)
    cos_day_of_year = np.cos(2 * np.pi * day_of_year / 365)
    sin_day_of_week = np.sin(2 * np.pi * day_of_week / 7)
    cos_day_of_week = np.cos(2 * np.pi * day_of_week / 7)
    sin_month = np.sin(2 * np.pi * month / 12)
    cos_month = np.cos(2 * np.pi * month / 12)

    # Create feature array of 6 values and reshape to (1, 6, 1)
    features = np.array([sin_day_of_year, cos_day_of_year,
                         sin_day_of_week, cos_day_of_week,
                         sin_month, cos_month], dtype=np.float32)
    features = torch.tensor(features).unsqueeze(0).unsqueeze(-1)

    model.eval()
    with torch.no_grad():
        prediction = model(features)
    return prediction.numpy()[0]

# Example: load the updated model for inference.
model_infer = WeatherTransformer(emb_dim=emb_dim, nhead=nhead, num_layers=num_layers, dropout=dropout, output_dim=3)  # Must use same hyperparameters as trained model.
model_infer.load_state_dict(torch.load('weather_transformer.pt'))
model_infer.eval()

# Predict weather for a sample date.
sample_date = "2025-03-28"
pred = predict_weather(model_infer, sample_date)
print(f"Predicted Maximum Temperature: {pred[0]:.2f}°C, Minimum Temperature: {pred[1]:.2f}°C, Maximum Wind Speed: {pred[2]:.2f}")

def compare_predictions(model_pred, api_data, target_mean, target_std):
    """
    Compare the extremeness of the model prediction with the API forecast.

    For temperature, we:
      - Compute the model's average temperature = (predicted_max + predicted_min)/2
      - Compute the API's average temperature from its low and high values.
      - Compute a z-score for temperature using the average of the training means and stds for max and min temperature.

    For wind speed, we:
      - Compare the model's predicted max wind speed with the API's reported high wind speed.
      - Compute a z-score using the training mean and std for wind speed.

    The function prints the z-scores and indicates which forecast is more extreme for each metric.
    Returns a dictionary with keys "temperature" and "wind" indicating the source ("model" or "api")
    that predicts a more extreme value.
    """
    # Temperature comparison:
    model_avg_temp = (model_pred[0] + model_pred[1]) / 2.0
    api_temp_low = api_data[0]['general']['temperature']['low']
    api_temp_high = api_data[0]['general']['temperature']['high']
    api_avg_temp = (api_temp_low + api_temp_high) / 2.0

    # Use the average of the training means and stds for max and min temperature.
    ref_temp_mean = (target_mean[0].item() + target_mean[1].item()) / 2.0
    ref_temp_std = (target_std[0].item() + target_std[1].item()) / 2.0

    model_temp_z = abs(model_avg_temp - ref_temp_mean) / ref_temp_std
    api_temp_z = abs(api_avg_temp - ref_temp_mean) / ref_temp_std

    # Wind speed comparison:
    model_wind = model_pred[2]
    api_wind = api_data[0]['general']['wind']['speed']['high']

    ref_wind_mean = target_mean[2].item()
    ref_wind_std = target_std[2].item()

    model_wind_z = abs(model_wind - ref_wind_mean) / ref_wind_std
    api_wind_z = abs(api_wind - ref_wind_mean) / ref_wind_std

    print(f"Temperature - Model Avg: {model_avg_temp:.2f}, API Avg: {api_avg_temp:.2f}")
    print(f"Temperature Z-Score: Model: {model_temp_z:.2f}, API: {api_temp_z:.2f}")
    print(f"Wind Speed - Model: {model_wind:.2f}, API: {api_wind:.2f}")
    print(f"Wind Speed Z-Score: Model: {model_wind_z:.2f}, API: {api_wind_z:.2f}")

    result = {}
    if model_temp_z > api_temp_z:
        print("Model predicts a more extreme temperature forecast.")
        result["temperature"] = "model"
    else:
        print("API predicts a more extreme temperature forecast.")
        result["temperature"] = "api"

    if model_wind_z > api_wind_z:
        print("Model predicts a more extreme wind speed forecast.")
        result["wind"] = "model"
    else:
        print("API predicts a more extreme wind speed forecast.")
        result["wind"] = "api"

    return result

# Sample API data (as provided):
sample_api_data = [{
    'date': '2025-03-28',
    'updatedTimestamp': '2025-03-28T17:40:45+08:00',
    'general': {
        'temperature': {
            'low': 25,
            'high': 33,
            'unit': 'Degrees Celsius'
        },
        'relativeHumidity': {
            'low': 60,
            'high': 95,
            'unit': 'Percentage'
        },
        'forecast': {
            'code': 'TL',
            'text': 'Thundery Showers'
        },
        'validPeriod': {
            'start': '2025-03-28T18:00:00+08:00',
            'end': '2025-03-29T18:00:00+08:00',
            'text': '6 PM 28 Mar to 6 PM 29 Mar'
        },
        'wind': {
            'speed': {
                'low': 5,
                'high': 15
            },
            'direction': 'NNW'
        }
    },
    'periods': [
        {
            'timePeriod': {
                'start': '2025-03-28T18:00:00+08:00',
                'end': '2025-03-29T06:00:00+08:00',
                'text': '6 pm 28 Mar to 6 am 29 Mar'
            },
            'regions': {
                'west': {'code': 'CL', 'text': 'Cloudy'},
                'east': {'code': 'CL', 'text': 'Cloudy'},
                'central': {'code': 'CL', 'text': 'Cloudy'},
                'south': {'code': 'CL', 'text': 'Cloudy'},
                'north': {'code': 'CL', 'text': 'Cloudy'}
            }
        },
        {
            'timePeriod': {
                'start': '2025-03-29T06:00:00+08:00',
                'end': '2025-03-29T12:00:00+08:00',
                'text': '6 am to Midday 29 Mar'
            },
            'regions': {
                'west': {'code': 'PC', 'text': 'Partly Cloudy (Day)'},
                'east': {'code': 'PC', 'text': 'Partly Cloudy (Day)'},
                'central': {'code': 'PC', 'text': 'Partly Cloudy (Day)'},
                'south': {'code': 'PC', 'text': 'Partly Cloudy (Day)'},
                'north': {'code': 'PC', 'text': 'Partly Cloudy (Day)'}
            }
        },
        {
            'timePeriod': {
                'start': '2025-03-29T12:00:00+08:00',
                'end': '2025-03-29T18:00:00+08:00',
                'text': 'Midday to 6 pm 29 Mar'
            },
            'regions': {
                'west': {'code': 'TL', 'text': 'Thundery Showers'},
                'east': {'code': 'TL', 'text': 'Thundery Showers'},
                'central': {'code': 'TL', 'text': 'Thundery Showers'},
                'south': {'code': 'TL', 'text': 'Thundery Showers'},
                'north': {'code': 'TL', 'text': 'Thundery Showers'}
            }
        }
    ],
    'timestamp': '2025-03-28T17:31:00+08:00'
}]

# Example usage:
# Assume `pred` is the output from your model prediction function (i.e., [max_temp, min_temp, max_wind])
# and `target_mean` and `target_std` are computed from your training data.
result = compare_predictions(pred, sample_api_data, target_mean, target_std)
# result = compare_predictions(pred, api_data, target_mean, target_std)

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# -----------------------------
# 1. MLP Model Definition & Loss
# -----------------------------
class MLP(nn.Module):
    def __init__(self, input_dim=6, hidden_dim=64, num_layers=2, dropout=0.1, output_dim=3):
        """
        A simple multi-layer perceptron (MLP) model.
        - input_dim: Number of input features (6 cyclic features)
        - hidden_dim: Size of hidden layers
        - num_layers: Number of hidden layers
        - dropout: Dropout rate
        - output_dim: Number of output targets (3)
        """
        super(MLP, self).__init__()
        layers = []
        # Input layer
        layers.append(nn.Linear(input_dim, hidden_dim))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout))
        # Additional hidden layers
        for _ in range(num_layers - 1):
            layers.append(nn.Linear(hidden_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
        # Final output layer
        layers.append(nn.Linear(hidden_dim, output_dim))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        # x comes with shape (batch, seq_len, 1); flatten it to (batch, input_dim)
        x = x.view(x.size(0), -1)
        return self.net(x)

def combined_loss(pred, target, target_mean, target_std, penalty_factor=2.0, alpha=0.5):
    """
    Combined loss: weighted sum of a custom loss (penalizing errors on extreme targets)
    and standard MSE loss.
    """
    base_loss = (pred - target)**2
    mask = (torch.abs(target - target_mean) > target_std).float()
    custom_loss_val = (base_loss * (1 + penalty_factor * mask)).mean()
    mse_loss_val = nn.MSELoss()(pred, target)
    return alpha * custom_loss_val + (1 - alpha) * mse_loss_val

# -----------------------------
# 2. Hyperparameter Tuning with Optuna
# -----------------------------
import optuna
from sklearn.model_selection import KFold
from torch.utils.data import Subset, DataLoader

def objective(trial):
    # Sample hyperparameters for the MLP.
    hidden_dim = trial.suggest_int("hidden_dim", 16, 128)
    num_layers = trial.suggest_int("num_layers", 1, 3)
    dropout = trial.suggest_float("dropout", 0.0, 0.5)
    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)
    alpha = trial.suggest_float("alpha", 0.3, 0.7)
    penalty_factor = trial.suggest_float("penalty_factor", 1.0, 3.0)

    # Set up 5-fold cross validation.
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    val_losses = []

    for train_index, val_index in kf.split(np.arange(len(dataset))):
        train_subset = Subset(dataset, train_index)
        val_subset = Subset(dataset, val_index)

        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)

        # Instantiate MLP with current hyperparameters.
        model = MLP(input_dim=6, hidden_dim=hidden_dim, num_layers=num_layers,
                    dropout=dropout, output_dim=3)
        optimizer = optim.Adam(model.parameters(), lr=lr)

        # Train for a few epochs on the training fold.
        epochs = 20
        for epoch in range(epochs):
            model.train()
            for features, targets in train_loader:
                optimizer.zero_grad()
                outputs = model(features)
                loss = combined_loss(outputs, targets, target_mean, target_std,
                                     penalty_factor, alpha)
                loss.backward()
                optimizer.step()

        # Evaluate on the validation fold.
        model.eval()
        total_val_loss = 0.0
        with torch.no_grad():
            for features, targets in val_loader:
                outputs = model(features)
                loss = combined_loss(outputs, targets, target_mean, target_std,
                                     penalty_factor, alpha)
                total_val_loss += loss.item() * features.size(0)
        avg_val_loss = total_val_loss / len(val_subset)
        val_losses.append(avg_val_loss)

    return np.mean(val_losses)

# Run hyperparameter tuning
study = optuna.create_study(direction="minimize", pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=20)

print("Best trial:")
trial = study.best_trial
for key, value in trial.params.items():
    print(f"  {key}: {value}")

# -----------------------------
# 3. Final Training with Plotting (Loss, RMSE, MAPE)
# -----------------------------
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Use best hyperparameters from Optuna (or defaults if not available)
best_params = study.best_trial.params
hidden_dim = best_params.get("hidden_dim", 64)
num_layers = best_params.get("num_layers", 2)
dropout = best_params.get("dropout", 0.1)
lr = best_params.get("lr", 0.001)
alpha = best_params.get("alpha", 0.5)
penalty_factor = best_params.get("penalty_factor", 2.0)

# Create a fixed train-validation split (80-20).
indices = np.arange(len(dataset))
train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)

from torch.utils.data import Subset
train_subset = Subset(dataset, train_idx)
val_subset = Subset(dataset, val_idx)

train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)

# Instantiate the final MLP model.
model = MLP(input_dim=6, hidden_dim=hidden_dim, num_layers=num_layers,
            dropout=dropout, output_dim=3)
optimizer = optim.Adam(model.parameters(), lr=lr)

# Containers to store training metrics.
num_epochs = 100
train_losses = []
val_losses = []
train_rmse = []
val_rmse = []
train_mape = []
val_mape = []

def compute_mape(pred, target):
    epsilon = 1e-8  # avoid division by zero
    return torch.mean(torch.abs((pred - target) / (target + epsilon))).item()

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    running_mape = 0.0
    for features, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(features)
        loss = combined_loss(outputs, targets, target_mean, target_std,
                             penalty_factor, alpha)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * features.size(0)
        running_mape += compute_mape(outputs, targets) * features.size(0)
    avg_train_loss = running_loss / len(train_subset)
    train_losses.append(avg_train_loss)
    train_rmse.append(np.sqrt(avg_train_loss))
    train_mape.append(running_mape / len(train_subset))

    # Validation evaluation.
    model.eval()
    running_val_loss = 0.0
    running_val_mape = 0.0
    with torch.no_grad():
        for features, targets in val_loader:
            outputs = model(features)
            loss = combined_loss(outputs, targets, target_mean, target_std,
                                 penalty_factor, alpha)
            running_val_loss += loss.item() * features.size(0)
            running_val_mape += compute_mape(outputs, targets) * features.size(0)
    avg_val_loss = running_val_loss / len(val_subset)
    val_losses.append(avg_val_loss)
    val_rmse.append(np.sqrt(avg_val_loss))
    val_mape.append(running_val_mape / len(val_subset))

    if (epoch+1) % 10 == 0:
        print(f"Epoch {epoch+1}/{num_epochs}:")
        print(f"  Train Loss: {avg_train_loss:.4f} (RMSE: {train_rmse[-1]:.4f}, MAPE: {train_mape[-1]*100:.2f}%)")
        print(f"  Val Loss:   {avg_val_loss:.4f} (RMSE: {val_rmse[-1]:.4f}, MAPE: {val_mape[-1]*100:.2f}%)")

# Plot the training curves.
plt.figure(figsize=(14, 5))

plt.subplot(1, 3, 1)
plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs+1), val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(range(1, num_epochs+1), train_rmse, label='Train RMSE')
plt.plot(range(1, num_epochs+1), val_rmse, label='Val RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.title('RMSE over Epochs')
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(range(1, num_epochs+1), [m*100 for m in train_mape], label='Train MAPE (%)')
plt.plot(range(1, num_epochs+1), [m*100 for m in val_mape], label='Val MAPE (%)')
plt.xlabel('Epoch')
plt.ylabel('MAPE (%)')
plt.title('MAPE over Epochs')
plt.legend()

plt.tight_layout()
plt.show()

# Save the final model.
torch.save(model.state_dict(), 'mlp_model.pt')
print("Final model saved as 'mlp_model.pt'.")

def predict_weather(model, date):
    """
    Given a date string (e.g., "2025-03-28"), compute cyclic features for
    day-of-year, day-of-week, and month, then predict the outputs:
    maximum_temperature, minimum_temperature, and max_wind_speed.
    """
    date_obj = pd.to_datetime(date)
    day_of_year = date_obj.dayofyear
    day_of_week = date_obj.dayofweek
    month = date_obj.month

    sin_day_of_year = np.sin(2 * np.pi * day_of_year / 365)
    cos_day_of_year = np.cos(2 * np.pi * day_of_year / 365)
    sin_day_of_week = np.sin(2 * np.pi * day_of_week / 7)
    cos_day_of_week = np.cos(2 * np.pi * day_of_week / 7)
    sin_month = np.sin(2 * np.pi * month / 12)
    cos_month = np.cos(2 * np.pi * month / 12)

    # Create feature array and reshape to (1, 6, 1)
    features = np.array([sin_day_of_year, cos_day_of_year,
                         sin_day_of_week, cos_day_of_week,
                         sin_month, cos_month], dtype=np.float32)
    features = torch.tensor(features).unsqueeze(0).unsqueeze(-1)

    model.eval()
    with torch.no_grad():
        prediction = model(features)
    return prediction.numpy()[0]

# Example usage for inference:
# Instantiate the model with the same hyperparameters as used in final training.
model_infer = MLP(input_dim=6, hidden_dim=hidden_dim, num_layers=num_layers,
                  dropout=dropout, output_dim=3)
model_infer.load_state_dict(torch.load('mlp_model.pt'))
model_infer.eval()

sample_date = "2025-03-28"
pred = predict_weather(model_infer, sample_date)
print(f"Predicted Maximum Temperature: {pred[0]:.2f}°C, Minimum Temperature: {pred[1]:.2f}°C, Maximum Wind Speed: {pred[2]:.2f}")

result = compare_predictions(pred, sample_api_data, target_mean, target_std)